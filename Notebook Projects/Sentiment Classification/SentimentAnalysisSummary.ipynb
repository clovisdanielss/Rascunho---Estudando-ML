{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc01535c-8c89-40b3-9461-f618794f1850",
   "metadata": {},
   "source": [
    "### Objetivo\n",
    "\n",
    "Esse notebook é um resumo do anterior. Tem como objetivo deixar tudo mais sucinto.\n",
    "\n",
    "Esse notebook tem como objetivo realizar uma análise de sentimentos do dataset da [amazon](http://deepyeti.ucsd.edu/jianmo/amazon/index.html). Essa vai ser uma série de 10 projetos de NLP onde esse é o primeiro projeto. \n",
    "\n",
    "Análise de sentimento é um problema difícil, mas tratando-se de NLP é um dos problemas mais simples de serem resolvidos.\n",
    "Neste projeto, pretendo:\n",
    "\n",
    "    1 - Analisar os dados.\n",
    "    2 - Classificar revisões em boas ou ruins. \n",
    "        2.1 - Aplicar um modelo usando rede neural recorrente\n",
    "        2.2 - Aplicar um modelo usando LSTM\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd3af40-ca68-4b6c-bf03-182ec638122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01fe1465-95a9-4c01-abfa-a48eb71d5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_samples(reader, data, samples_max_size, data_size):\n",
    "    for _ in reader:\n",
    "        validate = data[\"overall\"].value_counts().sum()\n",
    "        if validate == data_size:\n",
    "            break\n",
    "        for i in range(1,6):\n",
    "            aux = _.groupby(\"overall\").filter(lambda x: pd.Series([i]).isin(x[\"overall\"]).all())[data.columns]\n",
    "            curr_class_size = data[\"overall\"].value_counts()[i]\n",
    "            if curr_class_size + aux.shape[0] < samples_max_size:\n",
    "                #adiciona\n",
    "                data = pd.concat([data, aux], axis = 0)\n",
    "            elif curr_class_size < samples_max_size:\n",
    "                #adiciona parcial\n",
    "                offset = curr_class_size + aux.shape[0] - samples_max_size\n",
    "                data = pd.concat([data, aux[offset:]], axis = 0)\n",
    "            else:\n",
    "                clear_output(wait=True)\n",
    "                print(data[\"overall\"].value_counts())\n",
    "                continue\n",
    "            clear_output(wait=True)\n",
    "            print(data[\"overall\"].value_counts())\n",
    "    return data\n",
    "\n",
    "def process_sentence(sentence, padding=30):\n",
    "    stopwords = nltk.corpus.stopwords\n",
    "    stemer = nltk.stem.PorterStemmer()\n",
    "    processed = nltk.word_tokenize(sentence[:padding])\n",
    "    processed = [stemer.stem(word) for word in processed if word not in stopwords.words(\"english\")] + padding * [\"<PAD>\"]\n",
    "    return processed[:padding]\n",
    "\n",
    "def make_vocabulary(corpus, padding=30, max_vocab_size=2000):\n",
    "    vocabulary = {'<PAD>':0, '<UNK>':1}\n",
    "    rvocabulary = {0:'<PAD>', 1:'<UNK>'}\n",
    "    fvocabulary = {'<PAD>':0, '<UNK>':0}\n",
    "    index = 2\n",
    "    for sentence in tqdm(corpus):\n",
    "        processed = process_sentence(sentence, padding=padding)\n",
    "        for word in processed:\n",
    "            if word not in vocabulary.keys():\n",
    "                vocabulary[word] = index\n",
    "                rvocabulary[index] = word\n",
    "                fvocabulary[word] = 1\n",
    "                index += 1\n",
    "            else:\n",
    "                fvocabulary[word] += 1\n",
    "    fvocabulary = dict(sorted(fvocabulary.items(), key=lambda item: item[1], reverse=True))\n",
    "    words_by_freq = list(fvocabulary.keys())[:max_vocab_size]\n",
    "    index = 2\n",
    "    aux = {'<PAD>':0, '<UNK>':1}\n",
    "    for word in vocabulary.keys():\n",
    "        if word in words_by_freq and word not in ['<PAD>', '<UNK>']:\n",
    "            aux[word] = index\n",
    "            index+=1\n",
    "    vocabulary = aux\n",
    "    return rvocabulary, vocabulary, fvocabulary\n",
    "\n",
    "def tokenize(sentence, vocabulary, padding = 30):\n",
    "    sentence = process_sentence(sentence, padding=padding)\n",
    "    return [vocabulary[word] if word in vocabulary else vocabulary[\"<UNK>\"] for word in sentence]\n",
    "\n",
    "def detokenize(sentence, rvocabulary, padding=30):\n",
    "    return [rvocabulary[token] for token in sentence]\n",
    "\n",
    "class LogSoftmax(tf.keras.layers.Softmax):\n",
    "    def __init__(self):\n",
    "        super(LogSoftmax, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.math.log(super(LogSoftmax, self).call(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0081996-d0c0-4838-bc84-448cd815dbac",
   "metadata": {},
   "source": [
    "#### As funções acima\n",
    "\n",
    "As funções acima são justamente para se trabalhar os dados. Vão ser usadas abaixo antes do modelo ser executado.\n",
    "Para o processamento das sentenças foi criado esse método para gerar a sentença sem as palavras que são irrelevantes para análise de sentimento (stopwords) e também foi usada uma técnica de stem para preservar somente o radical aproximado das palavras.\n",
    "\n",
    "#### A seguir, initialize_data é a função que já trabalha os dados com alguns parâmetros\n",
    "\n",
    "Essa função, retorna os dados de treino e teste divididos. Também é lá que acontece o trabalho nos dados. Ou seja, a equalização das amostras de cada classe. A remoção de classes que se deseja tirar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75733409-7aff-4277-8135-82e7bedd6264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_data(chunksize, vocab_size, padding, proportion_train, exclude_classes = []):\n",
    "    data_size = 5*chunksize\n",
    "\n",
    "    reader = pd.read_json(\"./dataset/Video_Games.json\", chunksize=chunksize, lines=True)\n",
    "\n",
    "    data = []\n",
    "    for _ in reader:\n",
    "        data.append(_)\n",
    "        break\n",
    "    data = data[0]\n",
    "\n",
    "    samples_max_size = chunksize\n",
    "\n",
    "    data = equalize_samples(reader,data,samples_max_size,data_size)\n",
    "\n",
    "    for i in exclude_classes:\n",
    "        data = data[data.overall != i]\n",
    "    \n",
    "    data.dropna(subset=[\"reviewText\"],  axis=0, inplace=True)\n",
    "    clear_output(wait=True)\n",
    "    print(data[\"overall\"].value_counts())\n",
    "\n",
    "    vocab_size=vocab_size\n",
    "    padding=padding\n",
    "    rvocabulary, vocabulary, fvocabulary = make_vocabulary(data[\"reviewText\"].to_numpy(), padding=padding, max_vocab_size=vocab_size)\n",
    "    x = data[\"reviewText\"].to_numpy()\n",
    "    x = np.array([tokenize(text, vocabulary, padding=padding) for text in tqdm(x)])\n",
    "    print(x[:5])\n",
    "    y = pd.get_dummies(data[\"overall\"].to_numpy()).to_numpy() #Corrigindo intervalo para {0,1}\n",
    "    print(y[:5])\n",
    "    return sklearn.model_selection.train_test_split(x, y, train_size=proportion_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1480fa3e-dd00-480c-b8d6-95e398141900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    5000\n",
      "1    4995\n",
      "Name: overall, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9995/9995 [00:22<00:00, 444.98it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 9995/9995 [00:23<00:00, 432.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4  5  6  7  8  9  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [10 11 12 13 14  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [15 16 17 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [19 20 21 22 23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 2 24 25 26  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size=8000\n",
    "samples_size=5000\n",
    "padding=50\n",
    "xtrain, xtest, ytrain, ytest = initialize_data(samples_size, vocab_size, padding, .9, [2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d416e98-14d6-46e2-ba37-266888d92449",
   "metadata": {},
   "source": [
    "#### My Model\n",
    "\n",
    "Essa é uma função que define um mesmo padrão de modelo. Fiz isso porque não estou testando a arquitetura, mas sim a camada recorrente.\n",
    "\n",
    "No final, eu imprimo a epoca(Ou pelo menos a ordem de impressão), precisão e a perca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01839b0e-0a6e-4916-a1fa-9676ca6a98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(name, output=2, embedding_dim=128,model_dim=128, epochs=50, patience=3):\n",
    "    rlayer = None\n",
    "    if name == \"RNN\":\n",
    "        rlayer = tf.keras.layers.SimpleRNN(model_dim)\n",
    "    if name == \"LSTM\":\n",
    "        rlayer = tf.keras.layers.LSTM(model_dim)\n",
    "    if name == \"GRU\":\n",
    "        rlayer = tf.keras.layers.GRU(model_dim)\n",
    "    rnn = tf.keras.models.Sequential(\n",
    "        [\n",
    "            #Lembrando que são 5000 tokens + 2 que é o <PAD> e <UNK>\n",
    "            tf.keras.layers.Embedding(vocab_size + 2, embedding_dim, input_length=xtrain.shape[1]),\n",
    "            rlayer,\n",
    "            tf.keras.layers.Dense(output),\n",
    "            LogSoftmax()\n",
    "        ]\n",
    "    )\n",
    "    print(rnn.summary())\n",
    "\n",
    "    rnn.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = rnn.fit(\n",
    "        x = xtrain,\n",
    "        y = ytrain, \n",
    "        epochs=epochs, \n",
    "        batch_size=64,\n",
    "        validation_data=(xtest,ytest),\n",
    "        verbose=True,\n",
    "        callbacks=EarlyStopping(monitor='val_accuracy', mode='max',patience=patience)\n",
    "    ),\n",
    "\n",
    "    history = history[0]\n",
    "    for i in range(1,6):\n",
    "        print(\"Epoca:{0}, Acurácia:{1}, Acurácia_Val:{2}, Perca:{3}\".format(epochs-i+1,history.history[\"accuracy\"][-i],history.history[\"val_accuracy\"][-i], history.history[\"loss\"][-i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddfdae0d-564c-4fa0-97a5-711050021725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 512)           4097024   \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 128)               82048     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "log_softmax_1 (LogSoftmax)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 4,179,330\n",
      "Trainable params: 4,179,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "141/141 [==============================] - 13s 91ms/step - loss: 0.5645 - accuracy: 0.7164 - val_loss: 0.5207 - val_accuracy: 0.7880\n",
      "Epoch 2/50\n",
      "141/141 [==============================] - 13s 92ms/step - loss: 0.4532 - accuracy: 0.8148 - val_loss: 0.4580 - val_accuracy: 0.8020\n",
      "Epoch 3/50\n",
      "141/141 [==============================] - 13s 93ms/step - loss: 0.3874 - accuracy: 0.8506 - val_loss: 0.4856 - val_accuracy: 0.7860\n",
      "Epoch 4/50\n",
      "141/141 [==============================] - 13s 89ms/step - loss: 0.3544 - accuracy: 0.8667 - val_loss: 0.4946 - val_accuracy: 0.7920\n",
      "Epoch 5/50\n",
      "141/141 [==============================] - 13s 91ms/step - loss: 0.3510 - accuracy: 0.8625 - val_loss: 0.5109 - val_accuracy: 0.7860\n",
      "Epoca:50, Acurácia:0.8624791502952576, Acurácia_Val:0.7860000133514404, Perca:0.3510028123855591\n",
      "Epoca:49, Acurácia:0.8667037487030029, Acurácia_Val:0.7919999957084656, Perca:0.35443150997161865\n",
      "Epoca:48, Acurácia:0.8505836725234985, Acurácia_Val:0.7860000133514404, Perca:0.3874378502368927\n",
      "Epoca:47, Acurácia:0.8147860169410706, Acurácia_Val:0.8019999861717224, Perca:0.4531722366809845\n",
      "Epoca:46, Acurácia:0.7163980007171631, Acurácia_Val:0.7879999876022339, Perca:0.5644521117210388\n"
     ]
    }
   ],
   "source": [
    "my_model(\"RNN\", embedding_dim = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c09ae70e-9bc5-4350-bad8-6bc868b5430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 50, 64)            512128    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 50, 256)           82176     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "log_softmax_7 (LogSoftmax)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,120,130\n",
      "Trainable params: 1,120,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "141/141 [==============================] - 32s 216ms/step - loss: 0.6350 - accuracy: 0.6555 - val_loss: 0.5565 - val_accuracy: 0.7130\n",
      "Epoch 2/50\n",
      "141/141 [==============================] - 31s 221ms/step - loss: 0.5326 - accuracy: 0.7526 - val_loss: 0.4943 - val_accuracy: 0.7900\n",
      "Epoch 3/50\n",
      "141/141 [==============================] - 32s 228ms/step - loss: 0.4980 - accuracy: 0.7801 - val_loss: 0.7667 - val_accuracy: 0.4700\n",
      "Epoch 4/50\n",
      "141/141 [==============================] - 31s 222ms/step - loss: 0.6301 - accuracy: 0.6460 - val_loss: 0.5406 - val_accuracy: 0.7310\n",
      "Epoch 5/50\n",
      "141/141 [==============================] - 31s 222ms/step - loss: 0.5211 - accuracy: 0.7565 - val_loss: 0.5073 - val_accuracy: 0.7580\n",
      "Epoca:50, Acurácia:0.7565314173698425, Acurácia_Val:0.7580000162124634, Perca:0.521058976650238\n",
      "Epoca:49, Acurácia:0.6460255980491638, Acurácia_Val:0.7310000061988831, Perca:0.6301158666610718\n",
      "Epoca:48, Acurácia:0.7801000475883484, Acurácia_Val:0.4699999988079071, Perca:0.4980016052722931\n",
      "Epoca:47, Acurácia:0.7526403665542603, Acurácia_Val:0.7900000214576721, Perca:0.5325679183006287\n",
      "Epoca:46, Acurácia:0.6554752588272095, Acurácia_Val:0.7129999995231628, Perca:0.6350200772285461\n"
     ]
    }
   ],
   "source": [
    "my_model(\"RNN\",embedding_dim = 64, model_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cae4da7-ea5f-4810-a087-f7dee75b7e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 50, 128)           1024256   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "log_softmax_18 (LogSoftmax)  (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,123,586\n",
      "Trainable params: 1,123,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoca:50, Acurácia:0.4950528144836426, Acurácia_Val:0.49000000953674316, Perca:0.6933028697967529\n",
      "Epoca:49, Acurácia:0.4930517077445984, Acurácia_Val:0.49000000953674316, Perca:0.6933252215385437\n",
      "Epoca:48, Acurácia:0.49749860167503357, Acurácia_Val:0.49000000953674316, Perca:0.6932407021522522\n",
      "Epoca:47, Acurácia:0.502723753452301, Acurácia_Val:0.5099999904632568, Perca:0.6933501958847046\n",
      "Epoca:46, Acurácia:0.5055030584335327, Acurácia_Val:0.49000000953674316, Perca:0.6932137608528137\n"
     ]
    }
   ],
   "source": [
    "my_model(\"GRU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c8222c-438d-4651-9356-05e929bdaebc",
   "metadata": {},
   "source": [
    "#### Conclusão sobre a classificação binária\n",
    "##### Considerando \n",
    "O modelo mais simples obteve maior acurácia. 79%\n",
    "O modelo que usa LSTM obteve 72% de acurácia.\n",
    "O modelo que usa GRU ficou estagnado, e foi muito pobre.\n",
    "\n",
    "Foi considerado:\n",
    "\n",
    "```python\n",
    "vocab_size=8000\n",
    "samples_size=5000\n",
    "padding=30\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c62ff-f66f-41d7-a74b-4d9b78af22d7",
   "metadata": {},
   "source": [
    "#### Agora vamos ver os resultados obtidos com as 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c44c9afe-2f88-4cb2-8c18-43cbc11cfcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    3000\n",
      "5    3000\n",
      "2    3000\n",
      "3    2999\n",
      "1    2996\n",
      "Name: overall, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 14995/14995 [00:28<00:00, 517.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 14995/14995 [00:28<00:00, 517.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4  5  6  7  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 9  5 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 13 14 15  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 9 16 17 18 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 20 21 13  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "[[1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=3000\n",
    "samples_size=3000\n",
    "padding=40\n",
    "xtrain, xtest, ytrain, ytest = initialize_data(samples_size, vocab_size, padding, .9, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f6e43e2-6d41-40e1-afb8-c23f0be8e04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 128)           384256    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "log_softmax_2 (LogSoftmax)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 417,797\n",
      "Trainable params: 417,797\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoca:50, Acurácia:0.7315301895141602, Acurácia_Val:0.3226666748523712, Perca:0.7267200350761414\n",
      "Epoca:49, Acurácia:0.7199703454971313, Acurácia_Val:0.3580000102519989, Perca:0.7707535028457642\n",
      "Epoca:48, Acurácia:0.7029269933700562, Acurácia_Val:0.3306666612625122, Perca:0.8022030591964722\n",
      "Epoca:47, Acurácia:0.6829937100410461, Acurácia_Val:0.3513333201408386, Perca:0.8568177223205566\n",
      "Epoca:46, Acurácia:0.6529825925827026, Acurácia_Val:0.3473333418369293, Perca:0.932525634765625\n"
     ]
    }
   ],
   "source": [
    "my_model(\"RNN\", output=5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b77d9cfb-4181-4bd2-83d9-409cf4335283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 128)           384256    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "log_softmax_3 (LogSoftmax)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 516,485\n",
      "Trainable params: 516,485\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoca:50, Acurácia:0.19473879039287567, Acurácia_Val:0.21466666460037231, Perca:1.6100106239318848\n",
      "Epoca:49, Acurácia:0.19881437718868256, Acurácia_Val:0.21466666460037231, Perca:1.6099098920822144\n",
      "Epoca:48, Acurácia:0.19570210576057434, Acurácia_Val:0.1886666715145111, Perca:1.6100199222564697\n",
      "Epoca:47, Acurácia:0.19422008097171783, Acurácia_Val:0.21466666460037231, Perca:1.6102303266525269\n",
      "Epoca:46, Acurácia:0.20155613124370575, Acurácia_Val:0.18799999356269836, Perca:1.6042742729187012\n"
     ]
    }
   ],
   "source": [
    "my_model(\"LSTM\", output=5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb48af-da32-4c1f-bb84-647b37c0aaae",
   "metadata": {},
   "source": [
    "#### Os outros modelos performam bem pior. \n",
    "\n",
    "Não consegui parametrizar ou melhorar os resultados usando GRU ou LSTM. O que performou melhor para fazer essa classificação binária foi o RNN simples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5021b59-b5d6-464e-be78-9cd95b8e9657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
