{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a36a2e-96dd-4624-96ec-50621b3f00a5",
   "metadata": {},
   "source": [
    "### Objetivo\n",
    "\n",
    "Esse notebook tem como objetivo realizar uma análise de sentimentos do dataset da [amazon](http://deepyeti.ucsd.edu/jianmo/amazon/index.html). Essa vai ser uma série de 10 projetos de NLP onde esse é o primeiro projeto. \n",
    "\n",
    "Análise de sentimento é um problema difícil, mas tratando-se de NLP é um dos problemas mais simples de serem resolvidos.\n",
    "Neste projeto, pretendo:\n",
    "\n",
    "    1 - Analisar os dados.\n",
    "    2 - Aplicar um modelo usando rede neural recorrente\n",
    "    3 - Aplicar um modelo usando LSTM\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e69eb508-6de7-4cbf-a617-2776505d5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "chunksize = 10000\n",
    "data_size = 50000\n",
    "reader = pd.read_json(\"./dataset/Video_Games.json\", chunksize=chunksize, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6675c2ee-96a7-41e1-a995-8d29a379b3fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dados a serem usados\n",
    "\n",
    "Bem, essa base de dados tem 1.6 GB de dados. Não tenho como guardar tudo isso em memória, então decidi só carregar Nk amostras. A seguir, eu vou carregar mais e garantir que existam pelo menos Nk frases de cada nota. Esse numero de 5Nk amostras é um número que acredito que será possível ter um experimento interessante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4645d215-d8b2-4819-ac9a-17617b958955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       overall  verified   reviewTime      reviewerID        asin  \\\n",
      "10000        1      True   12 5, 2014   A5Z8JVR5415AR  B00000JDFT   \n",
      "10001        1      True  10 19, 2014   AMNHAB7BQ3BH9  B00000JDFT   \n",
      "10002        5      True  08 26, 2014  A140M20XMQANKX  B00000JDFT   \n",
      "10003        5      True   07 8, 2014   AIH4T1700UJ35  B00000JDFT   \n",
      "10004        5      True  12 17, 2013  A1YE2KK40M2TTG  B00000JDFT   \n",
      "\n",
      "               reviewerName  \\\n",
      "10000                 chris   \n",
      "10001  Albert L. Horney Jr.   \n",
      "10002          Michael Ault   \n",
      "10003        Chris Brunelle   \n",
      "10004                 Wilco   \n",
      "\n",
      "                                              reviewText  \\\n",
      "10000  Defective product---... Microsoft made these s...   \n",
      "10001     Could never get it to work on any of my games.   \n",
      "10002              well its a sidewinder what can ya say   \n",
      "10003                                              great   \n",
      "10004  Like many others, I consider this joystick to ...   \n",
      "\n",
      "                                  summary  unixReviewTime  vote style image  \n",
      "10000              Merchant is not honest      1417737600   NaN   NaN   NaN  \n",
      "10001                            One Star      1413676800   NaN   NaN   NaN  \n",
      "10002                          Five Stars      1409011200   NaN   NaN   NaN  \n",
      "10003                          Five Stars      1404777600   NaN   NaN   NaN  \n",
      "10004  The best flying joystick ever made      1387238400   2.0   NaN   NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['overall', 'verified', 'reviewTime', 'reviewerID', 'asin',\n",
       "       'reviewerName', 'reviewText', 'summary', 'unixReviewTime', 'vote',\n",
       "       'style', 'image'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for _ in reader:\n",
    "    data.append(_)\n",
    "    break\n",
    "data = data[0]\n",
    "print(data.head())\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b9bc4-9e59-40cb-aba6-8dfc0ed387d3",
   "metadata": {},
   "source": [
    "#### Percebemos que as notas não estão balanceadas.\n",
    "\n",
    "Isso pode acabar trazendo uma tendência nas avaliações. Vou tentar equilibrar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "aa11f094-1d7a-46d6-9024-f352df6b86a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    6678\n",
       "4    1443\n",
       "1     915\n",
       "3     636\n",
       "2     328\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"overall\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c994fc48-a634-4cf8-b288-2f2c56be82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_max_size = chunksize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d8656-509b-45af-93ac-fa95035610ef",
   "metadata": {},
   "source": [
    "#### Garantindo que existam samples_max_size amostras para cada nota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45014789-b246-421c-bd73-db749d92cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_samples(reader, data, samples_max_size, data_size):\n",
    "    for _ in reader:\n",
    "        validate = data[\"overall\"].value_counts().sum()\n",
    "        if validate == data_size:\n",
    "            break\n",
    "        for i in range(1,6):\n",
    "            aux = _.groupby(\"overall\").filter(lambda x: pd.Series([i]).isin(x[\"overall\"]).all())[data.columns]\n",
    "            curr_class_size = data[\"overall\"].value_counts()[i]\n",
    "            if curr_class_size + aux.shape[0] < samples_max_size:\n",
    "                #adiciona\n",
    "                data = pd.concat([data, aux], axis = 0)\n",
    "            elif curr_class_size < samples_max_size:\n",
    "                #adiciona parcial\n",
    "                offset = curr_class_size + aux.shape[0] - samples_max_size\n",
    "                data = pd.concat([data, aux[offset:]], axis = 0)\n",
    "            else:\n",
    "                clear_output(wait=True)\n",
    "                print(data[\"overall\"].value_counts())\n",
    "                continue\n",
    "            clear_output(wait=True)\n",
    "            print(data[\"overall\"].value_counts())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "7ac4765f-214e-4647-a6e1-15e89d636dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>06 9, 2014</td>\n",
       "      <td>A21ROB4YDOZA5P</td>\n",
       "      <td>0439381673</td>\n",
       "      <td>Mary M. Clark</td>\n",
       "      <td>I used to play this game years ago and loved i...</td>\n",
       "      <td>Did not like this</td>\n",
       "      <td>1402272000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>05 10, 2014</td>\n",
       "      <td>A3TNZ2Q5E7HTHD</td>\n",
       "      <td>0439381673</td>\n",
       "      <td>Sarabatya</td>\n",
       "      <td>The game itself worked great but the story lin...</td>\n",
       "      <td>Almost Perfect</td>\n",
       "      <td>1399680000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>02 7, 2014</td>\n",
       "      <td>A1OKRM3QFEATQO</td>\n",
       "      <td>0439381673</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>I had to learn the hard way after ordering thi...</td>\n",
       "      <td>DOES NOT WORK WITH MAC OS unless it is 10.3 or...</td>\n",
       "      <td>1391731200</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0        1      True   06 9, 2014  A21ROB4YDOZA5P  0439381673   \n",
       "1        3      True  05 10, 2014  A3TNZ2Q5E7HTHD  0439381673   \n",
       "2        4      True   02 7, 2014  A1OKRM3QFEATQO  0439381673   \n",
       "\n",
       "      reviewerName                                         reviewText  \\\n",
       "0    Mary M. Clark  I used to play this game years ago and loved i...   \n",
       "1        Sarabatya  The game itself worked great but the story lin...   \n",
       "2  Amazon Customer  I had to learn the hard way after ordering thi...   \n",
       "\n",
       "                                             summary  unixReviewTime  vote  \\\n",
       "0                                  Did not like this      1402272000   NaN   \n",
       "1                                     Almost Perfect      1399680000   NaN   \n",
       "2  DOES NOT WORK WITH MAC OS unless it is 10.3 or...      1391731200  15.0   \n",
       "\n",
       "  style image  \n",
       "0   NaN   NaN  \n",
       "1   NaN   NaN  \n",
       "2   NaN   NaN  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11e4ad-9c70-425b-a887-3c476222d52a",
   "metadata": {},
   "source": [
    "#### Lembre-se de remover os NaNs dos dados que serão usados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "04134841-ec74-4db6-a508-91c6de2ef67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall               0\n",
       "verified              0\n",
       "reviewTime            0\n",
       "reviewerID            0\n",
       "asin                  0\n",
       "reviewerName          2\n",
       "reviewText            6\n",
       "summary               7\n",
       "unixReviewTime        0\n",
       "vote              32765\n",
       "style             33126\n",
       "image             49739\n",
       "dtype: int64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "72ef9c47-61bd-4bde-b189-55c555173e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall               0\n",
       "verified              0\n",
       "reviewTime            0\n",
       "reviewerID            0\n",
       "asin                  0\n",
       "reviewerName          2\n",
       "reviewText            0\n",
       "summary               7\n",
       "unixReviewTime        0\n",
       "vote              32763\n",
       "style             33122\n",
       "image             49739\n",
       "dtype: int64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(subset=[\"reviewText\"],  axis=0, inplace=True)\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c489b1c-6b3e-4d29-98eb-ebfd14f04e96",
   "metadata": {},
   "source": [
    "#### Alguns elementos foram deletados, mas ainda temos uma distribuição legal entre as notas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "634a4e7e-1b7c-4477-a7e4-ba13146eb03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    10000\n",
       "5    10000\n",
       "2    10000\n",
       "3     9999\n",
       "1     9995\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"overall\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345c8ee-fd11-4c67-8b4b-af3c083ea166",
   "metadata": {},
   "source": [
    "#### Processando a sentença\n",
    "\n",
    "Aqui foi criado esse método para gerar a sentença sem as palavras que são irrelevantes para análise de sentimento (stopwords) e também foi usada uma técnica de stem para preservar somente o radical aproximado das palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ffae763-ca56-4d36-9f5f-20e21145263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'amaz', 'period', '.', 'i', 'creat', 'review', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "def process_sentence(sentence, padding=30):\n",
    "    stopwords = nltk.corpus.stopwords\n",
    "    stemer = nltk.stem.PorterStemmer()\n",
    "    processed = nltk.word_tokenize(sentence[:padding])\n",
    "    processed = [stemer.stem(word) for word in processed if word not in stopwords.words(\"english\")] + padding * [\"<PAD>\"]\n",
    "    return processed[:padding]\n",
    "\n",
    "print(process_sentence(data[\"reviewText\"][100], padding=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b60ab-b9ee-422e-a1dd-47696d5117d9",
   "metadata": {},
   "source": [
    "#### Tokenizando a sentença processada\n",
    "\n",
    "Aqui é gerado os tokens de um corpus. Isso é, aqui é construído o vocabulário. Com os dicionários resultantes é possível construir um método para tokenizar e detokenizar.\n",
    "\n",
    "Como as palavras de pausa foram removidas, a detokenização será incompleta. Isto significa que não será possível reconstruir a frase toda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7122c8e-8a16-4cd7-b239-0f66a68b2e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 328.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<UNK>', 2: 'game', 3: 'amaz', 4: 'period', 5: '.', 6: 'i', 7: 'creat', 8: 'review'}\n",
      "{'<PAD>': 0, '<UNK>': 1, 'game': 2, 'amaz': 3, 'period': 4}\n",
      "{'<PAD>': 43, 'game': 1, 'amaz': 1, 'period': 1, '.': 1, 'i': 1, 'creat': 1, 'review': 1, '<UNK>': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def make_vocabulary(corpus, padding=30, max_vocab_size=2000):\n",
    "    vocabulary = {'<PAD>':0, '<UNK>':1}\n",
    "    rvocabulary = {0:'<PAD>', 1:'<UNK>'}\n",
    "    fvocabulary = {'<PAD>':0, '<UNK>':0}\n",
    "    index = 2\n",
    "    for sentence in tqdm(corpus):\n",
    "        processed = process_sentence(sentence, padding=padding)\n",
    "        for word in processed:\n",
    "            if word not in vocabulary.keys():\n",
    "                vocabulary[word] = index\n",
    "                rvocabulary[index] = word\n",
    "                fvocabulary[word] = 1\n",
    "                index += 1\n",
    "            else:\n",
    "                fvocabulary[word] += 1\n",
    "    fvocabulary = dict(sorted(fvocabulary.items(), key=lambda item: item[1], reverse=True))\n",
    "    words_by_freq = list(fvocabulary.keys())[:max_vocab_size]\n",
    "    index = 2\n",
    "    aux = {'<PAD>':0, '<UNK>':1}\n",
    "    for word in vocabulary.keys():\n",
    "        if word in words_by_freq and word not in ['<PAD>', '<UNK>']:\n",
    "            aux[word] = index\n",
    "            index+=1\n",
    "    vocabulary = aux\n",
    "    return rvocabulary, vocabulary, fvocabulary\n",
    "\n",
    "vocab = make_vocabulary([data[\"reviewText\"][100]], padding=50, max_vocab_size=4)\n",
    "print(vocab[0])\n",
    "print(vocab[1])\n",
    "print(vocab[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f85fc08-b8bf-40ff-bc4d-218c479856ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['game', 'amaz', 'period', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(sentence, vocabulary, padding = 30):\n",
    "    sentence = process_sentence(sentence, padding=padding)\n",
    "    return [vocabulary[word] if word in vocabulary else vocabulary[\"<UNK>\"] for word in sentence]\n",
    "def detokenize(sentence, rvocabulary, padding=30):\n",
    "    return [rvocabulary[token] for token in sentence]\n",
    "tokenized = tokenize(data[\"reviewText\"][100], vocab[1], padding=50)\n",
    "print(tokenized)\n",
    "detokenized = detokenize(tokenized, vocab[0], padding=50)\n",
    "print(detokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f6fc4219-4138-4f01-b460-b15bc31ab3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 49994/49994 [02:03<00:00, 405.28it/s]\n"
     ]
    }
   ],
   "source": [
    "rvocabulary, vocabulary, fvocabulary = make_vocabulary(data[\"reviewText\"].to_numpy(), padding=50, max_vocab_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ea61b37f-11ac-4e64-a255-d0702adb09f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<PAD>', 0), ('<UNK>', 1), ('i', 2), ('use', 3), ('play', 4), ('game', 5), ('year', 6), ('ago', 7), ('love', 8), ('.', 9), ('the', 10), ('work', 11), ('great', 12), ('stori', 13), ('line', 14), ('vi', 15), ('learn', 16), ('hard', 17), ('way', 18), ('order', 19)]\n"
     ]
    }
   ],
   "source": [
    "print(list(vocabulary.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "ddb39228-361b-4725-a15d-87454c72ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 186, 258, 9, 2, 259, 260, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['game', 'there', 'dirt2', '.', 'i', 'pc', 'cross', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenize(data[\"reviewText\"][100], vocabulary, padding=50)\n",
    "print(tokenized)\n",
    "detokenized = detokenize(tokenized, rvocabulary, padding=50)\n",
    "print(detokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd70206f-91e7-4148-8f51-d80ab44fc2f2",
   "metadata": {},
   "source": [
    "#### Agora que o vocabulário foi criado\n",
    "\n",
    "Agora que o vocabulário foi criado, podemos preparar os dados para treinar o modelo. E por preparar os dados eu me refiro a fazer a divisão dos dados em treino e teste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "c9702d01-d8b2-4aa9-b6b3-88bc565d5f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 49994/49994 [02:04<00:00, 401.99it/s]\n"
     ]
    }
   ],
   "source": [
    "x = data[\"reviewText\"].to_numpy()\n",
    "x = np.array([tokenize(text, vocabulary, padding=50) for text in tqdm(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "15d8757c-d519-4d3c-9057-38584b30b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"overall\"].to_numpy() - 1 #Corrigindo intervalo para [0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "d59842e9-b162-4d25-b06e-c4a688528984",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = sklearn.model_selection.train_test_split(x, y, train_size=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "1dd0f50b-81b2-4c8b-8cfe-431c84ade474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44994, 50), (5000, 50), (44994,), (5000,))"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, xtest.shape, ytrain.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eebb5c-605f-4b8c-9c00-2ab1d305986f",
   "metadata": {},
   "source": [
    "#### Agora os dados estão 100%\n",
    "\n",
    "Agora os dados estão 100% preparados para entrar em qualquer modelo. A etapa a seguir vai ser a de construir o modelo que vamos usar aqui. \n",
    "\n",
    "Observação:\n",
    "No modelo abaixo usei logsoftmax ao invés de softmax. Isso ajuda a previnir operar em cima de produtos de valores muito pequenos. No caso, trabalhando com logit teremos somatório de valores, que ajuda a estabilizar o aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47720a32-605b-489d-8341-e4c06a3f460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftmax(tf.keras.layers.Softmax):\n",
    "    def __init__(self):\n",
    "        super(LogSoftmax, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.math.log(super(LogSoftmax, self).call(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "4e6c5b5a-3d8f-441d-9030-c829cf4cd7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 50, 128)           640256    \n",
      "_________________________________________________________________\n",
      "simple_rnn_20 (SimpleRNN)    (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "log_softmax_3 (LogSoftmax)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 673,797\n",
      "Trainable params: 673,797\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size=5000\n",
    "embedding_dim=128\n",
    "rnn = tf.keras.models.Sequential(\n",
    "    [\n",
    "        #Lembrando que são 5000 tokens + 2 que é o <PAD> e <UNK>\n",
    "        tf.keras.layers.Embedding(vocab_size + 2, embedding_dim, input_length=x.shape[1]),\n",
    "        tf.keras.layers.SimpleRNN(128),\n",
    "        tf.keras.layers.Dropout(.4),\n",
    "        tf.keras.layers.Dense(5),\n",
    "        LogSoftmax()\n",
    "    ]\n",
    ")\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "e4da6769-9841-4e9e-b058-8b55f96af0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(optimizer='adam', \n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "3bec97e8-e808-4d66-8d63-24c256452b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "176/176 [==============================] - 23s 126ms/step - loss: 1.5205 - accuracy: 0.3098 - val_loss: 1.4332 - val_accuracy: 0.3658\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 23s 130ms/step - loss: 1.3786 - accuracy: 0.4051 - val_loss: 1.4216 - val_accuracy: 0.3772\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 23s 130ms/step - loss: 1.3005 - accuracy: 0.4525 - val_loss: 1.4094 - val_accuracy: 0.3888\n",
      "Epoch 4/20\n",
      "176/176 [==============================] - 23s 130ms/step - loss: 1.2097 - accuracy: 0.5066 - val_loss: 1.4658 - val_accuracy: 0.3700\n",
      "Epoch 5/20\n",
      "176/176 [==============================] - 22s 128ms/step - loss: 1.1343 - accuracy: 0.5493 - val_loss: 1.4960 - val_accuracy: 0.3734\n",
      "Epoch 6/20\n",
      "176/176 [==============================] - 22s 128ms/step - loss: 1.0719 - accuracy: 0.5806 - val_loss: 1.5528 - val_accuracy: 0.3636\n",
      "Epoch 7/20\n",
      "176/176 [==============================] - 23s 129ms/step - loss: 1.0265 - accuracy: 0.6031 - val_loss: 1.5794 - val_accuracy: 0.3616\n",
      "Epoch 8/20\n",
      "176/176 [==============================] - 22s 128ms/step - loss: 0.9779 - accuracy: 0.6243 - val_loss: 1.6558 - val_accuracy: 0.3494\n",
      "Epoch 9/20\n",
      "176/176 [==============================] - 23s 128ms/step - loss: 0.9429 - accuracy: 0.6405 - val_loss: 1.6653 - val_accuracy: 0.3750\n",
      "Epoch 10/20\n",
      "176/176 [==============================] - 22s 128ms/step - loss: 0.9101 - accuracy: 0.6556 - val_loss: 1.6938 - val_accuracy: 0.3626\n",
      "Epoch 11/20\n",
      "176/176 [==============================] - 22s 128ms/step - loss: 0.8760 - accuracy: 0.6715 - val_loss: 1.7945 - val_accuracy: 0.3356\n",
      "Epoch 12/20\n",
      "176/176 [==============================] - 22s 127ms/step - loss: 0.8402 - accuracy: 0.6849 - val_loss: 1.8547 - val_accuracy: 0.3384\n",
      "Epoch 13/20\n",
      "176/176 [==============================] - 23s 130ms/step - loss: 0.8069 - accuracy: 0.6977 - val_loss: 1.8594 - val_accuracy: 0.3582\n",
      "Epoch 14/20\n",
      "176/176 [==============================] - 23s 131ms/step - loss: 0.7707 - accuracy: 0.7158 - val_loss: 1.9262 - val_accuracy: 0.3416\n",
      "Epoch 15/20\n",
      "176/176 [==============================] - 23s 130ms/step - loss: 0.7391 - accuracy: 0.7259 - val_loss: 1.9726 - val_accuracy: 0.3480\n",
      "Epoch 16/20\n",
      "176/176 [==============================] - 23s 131ms/step - loss: 0.7137 - accuracy: 0.7387 - val_loss: 2.0128 - val_accuracy: 0.3524\n",
      "Epoch 17/20\n",
      "176/176 [==============================] - 23s 130ms/step - loss: 0.6730 - accuracy: 0.7565 - val_loss: 2.0441 - val_accuracy: 0.3542\n",
      "Epoch 18/20\n",
      "176/176 [==============================] - 25s 141ms/step - loss: 0.6419 - accuracy: 0.7683 - val_loss: 2.1009 - val_accuracy: 0.3572\n",
      "Epoch 19/20\n",
      "176/176 [==============================] - 24s 133ms/step - loss: 0.6187 - accuracy: 0.7765 - val_loss: 2.1791 - val_accuracy: 0.3548\n",
      "Epoch 20/20\n",
      "176/176 [==============================] - 24s 134ms/step - loss: 0.5862 - accuracy: 0.7875 - val_loss: 2.2082 - val_accuracy: 0.3536\n"
     ]
    }
   ],
   "source": [
    "history = rnn.fit(\n",
    "    x = xtrain,\n",
    "    y = ytrain, \n",
    "    epochs=20, \n",
    "    batch_size=256,\n",
    "    validation_data=(xtest,ytest)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73060fe2-048b-4ce0-8ee2-ad92c4ce9d12",
   "metadata": {},
   "source": [
    "#### A performance da RNN \n",
    "\n",
    "A performance da RNN foi bem ruim. Tendeu total ao overfitting. Isso quem sabe pode ser resolvido com mais dados. Também o padding que foi escolhido é muito pequeno.\n",
    "\n",
    "Eu ia até testar a LSTM, mas não faz muito sentido, afinal não estou usando sequências longas. Então vamos reconstruír os dados e usar o mesmo modelo. \n",
    "\n",
    "Também vou aproveitar para modificar o tamanho do vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "77a5cac3-adfd-4a2a-8a3d-3b25d4043456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 49994/49994 [05:15<00:00, 158.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 49994/49994 [05:07<00:00, 162.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 150, 128)          1024256   \n",
      "_________________________________________________________________\n",
      "simple_rnn_21 (SimpleRNN)    (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "log_softmax_4 (LogSoftmax)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,057,797\n",
      "Trainable params: 1,057,797\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "176/176 [==============================] - 60s 339ms/step - loss: 1.6158 - accuracy: 0.2027 - val_loss: 1.6134 - val_accuracy: 0.2010\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 62s 351ms/step - loss: 1.6106 - accuracy: 0.2039 - val_loss: 1.6117 - val_accuracy: 0.1872\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 63s 357ms/step - loss: 1.6103 - accuracy: 0.2020 - val_loss: 1.6099 - val_accuracy: 0.1980\n",
      "Epoch 4/20\n",
      "176/176 [==============================] - 63s 356ms/step - loss: 1.6104 - accuracy: 0.2021 - val_loss: 1.6096 - val_accuracy: 0.1896\n",
      "Epoch 5/20\n",
      "176/176 [==============================] - 67s 379ms/step - loss: 1.6100 - accuracy: 0.1976 - val_loss: 1.6109 - val_accuracy: 0.1896\n",
      "Epoch 6/20\n",
      "176/176 [==============================] - 66s 372ms/step - loss: 1.6101 - accuracy: 0.1993 - val_loss: 1.6094 - val_accuracy: 0.1958\n",
      "Epoch 7/20\n",
      "173/176 [============================>.] - ETA: 1s - loss: 1.6104 - accuracy: 0.2004"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14236/1943804411.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m history = rnn.fit(\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32md:\\python\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocab_size=8000\n",
    "padding=150\n",
    "rvocabulary, vocabulary, fvocabulary = make_vocabulary(data[\"reviewText\"].to_numpy(), paddingsamples_max_sizeng, max_vocab_size=vocab_size)\n",
    "x = data[\"reviewText\"].to_numpy()\n",
    "x = np.array([tokenize(text, vocabulary, padding=padding) for text in tqdm(x)])\n",
    "y = data[\"overall\"].to_numpy() - 1 #Corrigindo intervalo para [0,5)\n",
    "xtrain, xtest, ytrain, ytest = sklearn.model_selection.train_test_split(x, y, train_size=.9)\n",
    "\n",
    "embedding_dim=128\n",
    "rnn = tf.keras.models.Sequential(\n",
    "    [\n",
    "        #Lembrando que são 5000 tokens + 2 que é o <PAD> e <UNK>\n",
    "        tf.keras.layers.Embedding(vocab_size + 2, embedding_dim, input_length=x.shape[1]),\n",
    "        tf.keras.layers.SimpleRNN(128),\n",
    "        tf.keras.layers.Dropout(.4),\n",
    "        tf.keras.layers.Dense(5),\n",
    "        LogSoftmax()\n",
    "    ]\n",
    ")\n",
    "print(rnn.summary())\n",
    "\n",
    "rnn.compile(optimizer='adam', \n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = rnn.fit(\n",
    "    x = xtrain,\n",
    "    y = ytrain, \n",
    "    epochs=20, \n",
    "    batch_size=256,\n",
    "    validation_data=(xtest,ytest)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4577eb29-867c-4a6e-9303-9bb8ca6f0424",
   "metadata": {},
   "source": [
    "#### Infelizmente Parei a Execução\n",
    "\n",
    "Para minha infelicidade ficou tão ruim quanto o LSTM que eu antes havia testado. Ou seja, por mais que eu tenha recebido mais dados, não estou conseguindo generalizar bem com esses dados. \n",
    "\n",
    "Aumentar a riquesa do vocabulário se provou ineficiente. E aumentar o tamanho das sentenças também. E se aumentarmos o total de dados ? \n",
    "\n",
    "Abaixo eu tento fazer isso. E verifico com vocabulário de 5k e padding 50 se melhorou alguma coisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "a78f70a3-2457-440f-a8fb-b1b83ee69bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    25000\n",
      "3    25000\n",
      "4    25000\n",
      "5    25000\n",
      "2    25000\n",
      "Name: overall, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 124981/124981 [05:06<00:00, 407.57it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 124981/124981 [04:59<00:00, 417.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 50, 128)           640256    \n",
      "_________________________________________________________________\n",
      "simple_rnn_22 (SimpleRNN)    (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "log_softmax_5 (LogSoftmax)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 673,797\n",
      "Trainable params: 673,797\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "440/440 [==============================] - 53s 118ms/step - loss: 1.4671 - accuracy: 0.3438 - val_loss: 1.4022 - val_accuracy: 0.3824\n",
      "Epoch 2/20\n",
      "440/440 [==============================] - 55s 125ms/step - loss: 1.3833 - accuracy: 0.4010 - val_loss: 1.3960 - val_accuracy: 0.3960\n",
      "Epoch 3/20\n",
      "440/440 [==============================] - 51s 116ms/step - loss: 1.3432 - accuracy: 0.4275 - val_loss: 1.4152 - val_accuracy: 0.3841\n",
      "Epoch 4/20\n",
      "440/440 [==============================] - 54s 122ms/step - loss: 1.3052 - accuracy: 0.4523 - val_loss: 1.4271 - val_accuracy: 0.3816\n",
      "Epoch 5/20\n",
      "440/440 [==============================] - 53s 121ms/step - loss: 1.2680 - accuracy: 0.4750 - val_loss: 1.4314 - val_accuracy: 0.3881\n",
      "Epoch 6/20\n",
      "440/440 [==============================] - 53s 122ms/step - loss: 1.2388 - accuracy: 0.4931 - val_loss: 1.4567 - val_accuracy: 0.3710\n",
      "Epoch 7/20\n",
      "440/440 [==============================] - 53s 120ms/step - loss: 1.2085 - accuracy: 0.5103 - val_loss: 1.4743 - val_accuracy: 0.3822\n",
      "Epoch 8/20\n",
      "440/440 [==============================] - 52s 119ms/step - loss: 1.1821 - accuracy: 0.5241 - val_loss: 1.4902 - val_accuracy: 0.3696\n",
      "Epoch 9/20\n",
      "440/440 [==============================] - 54s 122ms/step - loss: 1.1581 - accuracy: 0.5384 - val_loss: 1.4905 - val_accuracy: 0.3817\n",
      "Epoch 10/20\n",
      "440/440 [==============================] - 52s 119ms/step - loss: 1.1311 - accuracy: 0.5522 - val_loss: 1.5395 - val_accuracy: 0.3606\n",
      "Epoch 11/20\n",
      "440/440 [==============================] - 54s 123ms/step - loss: 1.1098 - accuracy: 0.5635 - val_loss: 1.5214 - val_accuracy: 0.3821\n",
      "Epoch 12/20\n",
      "440/440 [==============================] - 58s 131ms/step - loss: 1.0852 - accuracy: 0.5754 - val_loss: 1.5638 - val_accuracy: 0.3711\n",
      "Epoch 13/20\n",
      "440/440 [==============================] - 57s 129ms/step - loss: 1.0582 - accuracy: 0.5890 - val_loss: 1.5986 - val_accuracy: 0.3699\n",
      "Epoch 14/20\n",
      "440/440 [==============================] - 56s 128ms/step - loss: 1.0351 - accuracy: 0.5989 - val_loss: 1.6265 - val_accuracy: 0.3613\n",
      "Epoch 15/20\n",
      "440/440 [==============================] - 53s 121ms/step - loss: 1.0090 - accuracy: 0.6126 - val_loss: 1.6427 - val_accuracy: 0.3606\n",
      "Epoch 16/20\n",
      "440/440 [==============================] - 55s 124ms/step - loss: 0.9834 - accuracy: 0.6251 - val_loss: 1.6715 - val_accuracy: 0.3651\n",
      "Epoch 17/20\n",
      "440/440 [==============================] - 53s 120ms/step - loss: 0.9585 - accuracy: 0.6359 - val_loss: 1.7114 - val_accuracy: 0.3535\n",
      "Epoch 18/20\n",
      "440/440 [==============================] - 53s 120ms/step - loss: 0.9337 - accuracy: 0.6479 - val_loss: 1.7292 - val_accuracy: 0.3503\n",
      "Epoch 19/20\n",
      "440/440 [==============================] - 53s 121ms/step - loss: 0.9050 - accuracy: 0.6597 - val_loss: 1.7095 - val_accuracy: 0.3683\n",
      "Epoch 20/20\n",
      "440/440 [==============================] - 58s 131ms/step - loss: 0.8814 - accuracy: 0.6715 - val_loss: 1.7766 - val_accuracy: 0.3487\n"
     ]
    }
   ],
   "source": [
    "chunksize = 25000\n",
    "data_size = 50000\n",
    "\n",
    "reader = pd.read_json(\"./dataset/Video_Games.json\", chunksize=chunksize, lines=True)\n",
    "\n",
    "data = []\n",
    "for _ in reader:\n",
    "    data.append(_)\n",
    "    break\n",
    "data = data[0]\n",
    "\n",
    "samples_max_size = chunksize\n",
    "\n",
    "for _ in reader:\n",
    "    validate = data[\"overall\"].value_counts().sum()\n",
    "    if validate == data_size:\n",
    "        break\n",
    "    for i in range(1,6):\n",
    "        aux = _.groupby(\"overall\").filter(lambda x: pd.Series([i]).isin(x[\"overall\"]).all())[data.columns]\n",
    "        curr_class_size = data[\"overall\"].value_counts()[i]\n",
    "        if curr_class_size + aux.shape[0] < samples_max_size:\n",
    "            #adiciona\n",
    "            data = pd.concat([data, aux], axis = 0)\n",
    "        elif curr_class_size < samples_max_size:\n",
    "            #adiciona parcial\n",
    "            offset = curr_class_size + aux.shape[0] - samples_max_size\n",
    "            data = pd.concat([data, aux[offset:]], axis = 0)\n",
    "        else:\n",
    "            clear_output(wait=True)\n",
    "            print(data[\"overall\"].value_counts())\n",
    "            continue\n",
    "        clear_output(wait=True)\n",
    "        print(data[\"overall\"].value_counts())\n",
    "\n",
    "data.dropna(subset=[\"reviewText\"],  axis=0, inplace=True)\n",
    "\n",
    "vocab_size=5000\n",
    "padding=50\n",
    "rvocabulary, vocabulary, fvocabulary = make_vocabulary(data[\"reviewText\"].to_numpy(), padding=padding, max_vocab_size=vocab_size)\n",
    "x = data[\"reviewText\"].to_numpy()\n",
    "x = np.array([tokenize(text, vocabulary, padding=padding) for text in tqdm(x)])\n",
    "y = data[\"overall\"].to_numpy() - 1 #Corrigindo intervalo para [0,5)\n",
    "xtrain, xtest, ytrain, ytest = sklearn.model_selection.train_test_split(x, y, train_size=.9)\n",
    "\n",
    "embedding_dim=128\n",
    "rnn = tf.keras.models.Sequential(\n",
    "    [\n",
    "        #Lembrando que são 5000 tokens + 2 que é o <PAD> e <UNK>\n",
    "        tf.keras.layers.Embedding(vocab_size + 2, embedding_dim, input_length=x.shape[1]),\n",
    "        tf.keras.layers.SimpleRNN(128),\n",
    "        tf.keras.layers.Dropout(.4),\n",
    "        tf.keras.layers.Dense(5),\n",
    "        LogSoftmax()\n",
    "    ]\n",
    ")\n",
    "print(rnn.summary())\n",
    "\n",
    "rnn.compile(optimizer='adam', \n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = rnn.fit(\n",
    "    x = xtrain,\n",
    "    y = ytrain, \n",
    "    epochs=20, \n",
    "    batch_size=256,\n",
    "    validation_data=(xtest,ytest)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9c9ba-7bb6-48d2-9675-87435617d202",
   "metadata": {},
   "source": [
    "#### Pelo visto...\n",
    "\n",
    "Mesmo aumentando a quantidade de dados não tivemos um resultado evidentemente melhor que o ultimo. A próxima estratégia que planejo é considerar o nome de quem fez o post como parte da mensagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de274b63-7d61-44e1-913e-fa2cff8aa825",
   "metadata": {},
   "source": [
    "#### Um passo pra trás\n",
    "\n",
    "Como eu não estive tendo bons resultados tomei a decisão de tentar fazer uma classificação binária. Já havia tentado transformar notas altas em boas e baixas em ruim, mas ainda assim ficou muito nebuloso.\n",
    "\n",
    "Vamos tentar aqui trabalhar agora somente com notas máximas e notas mínimas, e ver o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2365f11c-8038-4659-9818-93667dffefb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    5000\n",
      "1    4995\n",
      "Name: overall, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9995/9995 [00:23<00:00, 432.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 9995/9995 [00:22<00:00, 442.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4  5  6  7  8  9  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [10 11 12 13 14  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [15 16 17 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [19 20 21 22 23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 2 24 25 26  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]]\n",
      "[0 0 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunksize = 5000\n",
    "data_size = 5*chunksize\n",
    "\n",
    "reader = pd.read_json(\"./dataset/Video_Games.json\", chunksize=chunksize, lines=True)\n",
    "\n",
    "data = []\n",
    "for _ in reader:\n",
    "    data.append(_)\n",
    "    break\n",
    "data = data[0]\n",
    "\n",
    "samples_max_size = chunksize\n",
    "\n",
    "data = equalize_samples(reader,data,samples_max_size,data_size)\n",
    "\n",
    "data = data[data.overall != 2]\n",
    "data = data[data.overall != 3]\n",
    "data = data[data.overall != 4]\n",
    "data.dropna(subset=[\"reviewText\"],  axis=0, inplace=True)\n",
    "clear_output(wait=True)\n",
    "print(data[\"overall\"].value_counts())\n",
    "\n",
    "vocab_size=8000\n",
    "padding=50\n",
    "rvocabulary, vocabulary, fvocabulary = make_vocabulary(data[\"reviewText\"].to_numpy(), padding=padding, max_vocab_size=vocab_size)\n",
    "x = data[\"reviewText\"].to_numpy()\n",
    "x = np.array([tokenize(text, vocabulary, padding=padding) for text in tqdm(x)])\n",
    "print(x[:5])\n",
    "y = (data[\"overall\"].to_numpy() == 5)*1 #Corrigindo intervalo para {0,1}\n",
    "print(y[:5])\n",
    "xtrain, xtest, ytrain, ytest = sklearn.model_selection.train_test_split(x, y, train_size=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05f62631-6b45-4996-8db1-90c41a7fbfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 50, 128)           1024256   \n",
      "_________________________________________________________________\n",
      "simple_rnn_8 (SimpleRNN)     (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "log_softmax_7 (LogSoftmax)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,065,538\n",
      "Trainable params: 1,065,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 3s 75ms/step - loss: 0.6113 - accuracy: 0.6476 - val_loss: 0.5806 - val_accuracy: 0.7070\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 2s 69ms/step - loss: 0.4066 - accuracy: 0.8250 - val_loss: 0.4596 - val_accuracy: 0.8110\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 2s 69ms/step - loss: 0.3037 - accuracy: 0.8796 - val_loss: 0.5045 - val_accuracy: 0.7680\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 3s 70ms/step - loss: 0.2847 - accuracy: 0.8923 - val_loss: 0.4658 - val_accuracy: 0.8160\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 3s 70ms/step - loss: 0.2250 - accuracy: 0.9143 - val_loss: 0.5086 - val_accuracy: 0.7980\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 3s 71ms/step - loss: 0.1667 - accuracy: 0.9384 - val_loss: 0.5688 - val_accuracy: 0.8080\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 3s 74ms/step - loss: 0.1496 - accuracy: 0.9436 - val_loss: 0.5246 - val_accuracy: 0.7740\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 3s 74ms/step - loss: 0.1624 - accuracy: 0.9385 - val_loss: 0.6292 - val_accuracy: 0.7890\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 3s 73ms/step - loss: 0.1270 - accuracy: 0.9529 - val_loss: 0.5694 - val_accuracy: 0.7830\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 3s 72ms/step - loss: 0.1274 - accuracy: 0.9562 - val_loss: 0.6096 - val_accuracy: 0.7930\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 3s 74ms/step - loss: 0.1056 - accuracy: 0.9632 - val_loss: 0.6821 - val_accuracy: 0.7820\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 3s 74ms/step - loss: 0.1009 - accuracy: 0.9634 - val_loss: 0.7266 - val_accuracy: 0.7870\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 3s 76ms/step - loss: 0.0873 - accuracy: 0.9707 - val_loss: 0.7485 - val_accuracy: 0.7970\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 3s 75ms/step - loss: 0.0726 - accuracy: 0.9750 - val_loss: 0.9029 - val_accuracy: 0.7850\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 3s 74ms/step - loss: 0.0741 - accuracy: 0.9725 - val_loss: 0.8315 - val_accuracy: 0.7930\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 3s 74ms/step - loss: 0.0592 - accuracy: 0.9800 - val_loss: 0.9912 - val_accuracy: 0.8050\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 3s 72ms/step - loss: 0.0603 - accuracy: 0.9797 - val_loss: 0.8753 - val_accuracy: 0.7860\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 3s 74ms/step - loss: 0.0640 - accuracy: 0.9770 - val_loss: 0.7792 - val_accuracy: 0.7730\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 3s 73ms/step - loss: 0.0850 - accuracy: 0.9702 - val_loss: 0.7959 - val_accuracy: 0.7780\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 3s 76ms/step - loss: 0.0793 - accuracy: 0.9723 - val_loss: 0.8778 - val_accuracy: 0.7920\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=128\n",
    "rnn = tf.keras.models.Sequential(\n",
    "    [\n",
    "        #Lembrando que são 5000 tokens + 2 que é o <PAD> e <UNK>\n",
    "        tf.keras.layers.Embedding(vocab_size + 2, embedding_dim, input_length=x.shape[1]),\n",
    "        tf.keras.layers.SimpleRNN(128),\n",
    "        tf.keras.layers.Dropout(.4),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(.2),\n",
    "        tf.keras.layers.Dense(2),\n",
    "        LogSoftmax()\n",
    "    ]\n",
    ")\n",
    "print(rnn.summary())\n",
    "\n",
    "rnn.compile(optimizer='adam', \n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = rnn.fit(\n",
    "    x = xtrain,\n",
    "    y = ytrain, \n",
    "    epochs=20, \n",
    "    batch_size=256,\n",
    "    validation_data=(xtest,ytest)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad0d8b2-278a-4bfa-b116-5999c58e0ba0",
   "metadata": {},
   "source": [
    "#### Ok, agora tivemos um resultado. 78% de acurácia na validação (5000 vocab_size, 50 padding, 25k samples)\n",
    "\n",
    "Enquanto eu estava testando, mesmo somente com 2 classes, não conseguia progredir no treino. Só consegui fazer progresso quando eu adicionei uma nova camada ReLu entre a RNN e a definição de classes. \n",
    "\n",
    "E realmente, havia um defeito muito grande pois a ReLu força os valores a serem positivos. Sem fazer isso a gente vai acabar tirando log de valores negativos usando a LogSoftmax, causando assim erro.\n",
    "\n",
    "#### Se eu aumento os parâmetros a rede se torna ineficiente (8000 vocab_size, 150 padding, 5k samples)\n",
    "\n",
    "Essa ineficiência tem mais haver com o <b>padding</b>, visto que mantendo os outros parâmetros ainda foi possível atingir a mesma performance de <b>78% de acurácia na validação</b>\n",
    "\n",
    "Sei que isso acontece porque existem muitas revisões que contém um total de texto muito abaixo do padding. Será que uma LSTM seria capaz de resolver esse problema nessa mesma arquitetura ?\n",
    "\n",
    "#### Antes, vamos testar a LSTM com (8000 vocab_size, 50 padding, 5k samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16ca055e-8d17-4cbc-9e7f-574e2618413d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 50, 128)           1024256   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "log_softmax_10 (LogSoftmax)  (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,131,714\n",
      "Trainable params: 1,131,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=128\n",
    "rnn = tf.keras.models.Sequential(\n",
    "    [\n",
    "        #Lembrando que são 5000 tokens + 2 que é o <PAD> e <UNK>\n",
    "        tf.keras.layers.Embedding(vocab_size + 2, embedding_dim, input_length=x.shape[1]),\n",
    "        tf.keras.layers.GRU(128),\n",
    "        tf.keras.layers.Dropout(.4),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(.2),\n",
    "        tf.keras.layers.Dense(2),\n",
    "        LogSoftmax()\n",
    "    ]\n",
    ")\n",
    "print(rnn.summary())\n",
    "\n",
    "rnn.compile(optimizer='adam', \n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = rnn.fit(\n",
    "    x = xtrain,\n",
    "    y = ytrain, \n",
    "    epochs=20, \n",
    "    batch_size=256,\n",
    "    validation_data=(xtest,ytest),\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a73244-07b3-43f2-bc66-75e12abad41d",
   "metadata": {},
   "source": [
    "#### Parei a execução (Testei com LSTM e GRU)\n",
    "\n",
    "A LSTM não consegue performar tão bem nos dados com os parâmetros citados acima. Da mesma forma, a GRU não se comportou bem. Eu acredito que a dificuldade encontrada tá associada à rede. Porque com uma rede mais simples foi possível generalizar e conseguir uma acurácia de validação acima da média."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7c3fae4-b90b-4ff0-b333-f10e2f4c4ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49849915504455566"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"accuracy\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3acdfa-5660-4fa4-8ce2-5b2ffa5b8ba5",
   "metadata": {},
   "source": [
    "#### Vou tentar mecher na rede\n",
    "\n",
    "Vou ver se consigo generalizar com os mesmos dados, mas modificando a rede. Abaixo vou colocar somente a rede final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c72ca936-3242-4c39-b20e-3def86eac65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    5000\n",
      "1    4995\n",
      "Name: overall, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9995/9995 [00:16<00:00, 595.08it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 9995/9995 [00:15<00:00, 650.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4  5  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 7  8  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [10 11 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [13 14 15 16 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 2 18 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "chunksize = 5000\n",
    "data_size = 5*chunksize\n",
    "\n",
    "reader = pd.read_json(\"./dataset/Video_Games.json\", chunksize=chunksize, lines=True)\n",
    "\n",
    "data = []\n",
    "for _ in reader:\n",
    "    data.append(_)\n",
    "    break\n",
    "data = data[0]\n",
    "\n",
    "samples_max_size = chunksize\n",
    "\n",
    "data = equalize_samples(reader,data,samples_max_size,data_size)\n",
    "\n",
    "data = data[data.overall != 2]\n",
    "data = data[data.overall != 3]\n",
    "data = data[data.overall != 4]\n",
    "data.dropna(subset=[\"reviewText\"],  axis=0, inplace=True)\n",
    "clear_output(wait=True)\n",
    "print(data[\"overall\"].value_counts())\n",
    "\n",
    "vocab_size=8000\n",
    "padding=30\n",
    "rvocabulary, vocabulary, fvocabulary = make_vocabulary(data[\"reviewText\"].to_numpy(), padding=padding, max_vocab_size=vocab_size)\n",
    "x = data[\"reviewText\"].to_numpy()\n",
    "x = np.array([tokenize(text, vocabulary, padding=padding) for text in tqdm(x)])\n",
    "print(x[:5])\n",
    "y = pd.get_dummies((data[\"overall\"].to_numpy() == 5)*1).to_numpy() #Corrigindo intervalo para {0,1}\n",
    "print(y[:5])\n",
    "xtrain, xtest, ytrain, ytest = sklearn.model_selection.train_test_split(x, y, train_size=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ab111f5-5171-4cf0-adfe-18406e5d6a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 30, 128)           1024256   \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 200)               263200    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 1,287,858\n",
      "Trainable params: 1,287,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "141/141 [==============================] - 12s 76ms/step - loss: 0.6218 - accuracy: 0.6110 - val_loss: 0.4659 - val_accuracy: 0.7610\n",
      "Epoch 2/20\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.4255 - accuracy: 0.8116 - val_loss: 0.4851 - val_accuracy: 0.7660\n",
      "Epoch 3/20\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.3515 - accuracy: 0.8548 - val_loss: 0.4898 - val_accuracy: 0.7880\n",
      "Epoch 4/20\n",
      "141/141 [==============================] - 11s 75ms/step - loss: 0.3087 - accuracy: 0.8726 - val_loss: 0.4582 - val_accuracy: 0.7900\n",
      "Epoch 5/20\n",
      "141/141 [==============================] - 11s 77ms/step - loss: 0.2845 - accuracy: 0.8846 - val_loss: 0.4755 - val_accuracy: 0.7930\n",
      "Epoch 6/20\n",
      "141/141 [==============================] - 9s 66ms/step - loss: 0.2677 - accuracy: 0.8955 - val_loss: 0.5057 - val_accuracy: 0.7810\n",
      "Epoch 7/20\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.2487 - accuracy: 0.8977 - val_loss: 0.5366 - val_accuracy: 0.7820\n",
      "Epoch 8/20\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.2345 - accuracy: 0.9041 - val_loss: 0.5918 - val_accuracy: 0.7790\n",
      "Epoch 9/20\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.2216 - accuracy: 0.9048 - val_loss: 0.5443 - val_accuracy: 0.7780\n",
      "Epoch 10/20\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.2106 - accuracy: 0.9073 - val_loss: 0.5619 - val_accuracy: 0.7740\n",
      "Epoch 11/20\n",
      "141/141 [==============================] - 10s 74ms/step - loss: 0.1941 - accuracy: 0.9087 - val_loss: 0.6175 - val_accuracy: 0.7800\n",
      "Epoch 12/20\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.1868 - accuracy: 0.9130 - val_loss: 0.6092 - val_accuracy: 0.7680\n",
      "Epoch 13/20\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.1866 - accuracy: 0.9106 - val_loss: 0.5834 - val_accuracy: 0.7730\n",
      "Epoch 14/20\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.1725 - accuracy: 0.9187 - val_loss: 0.6062 - val_accuracy: 0.7560\n",
      "Epoch 15/20\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.1794 - accuracy: 0.9145 - val_loss: 0.6228 - val_accuracy: 0.7710\n",
      "Epoch 16/20\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.1691 - accuracy: 0.9211 - val_loss: 0.6973 - val_accuracy: 0.7700\n",
      "Epoch 17/20\n",
      "141/141 [==============================] - 11s 75ms/step - loss: 0.1596 - accuracy: 0.9258 - val_loss: 0.6577 - val_accuracy: 0.7670\n",
      "Epoch 18/20\n",
      "141/141 [==============================] - 10s 74ms/step - loss: 0.1483 - accuracy: 0.9350 - val_loss: 0.7478 - val_accuracy: 0.7600\n",
      "Epoch 19/20\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.1429 - accuracy: 0.9391 - val_loss: 0.9058 - val_accuracy: 0.7590\n",
      "Epoch 20/20\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.1354 - accuracy: 0.9399 - val_loss: 0.7988 - val_accuracy: 0.7620\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=128\n",
    "rnn = tf.keras.models.Sequential(\n",
    "    [\n",
    "        #Lembrando que são 5000 tokens + 2 que é o <PAD> e <UNK>\n",
    "        tf.keras.layers.Embedding(vocab_size + 2, embedding_dim, input_length=x.shape[1]),\n",
    "        tf.keras.layers.LSTM(200),\n",
    "        tf.keras.layers.Dense(2, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "print(rnn.summary())\n",
    "\n",
    "rnn.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = rnn.fit(\n",
    "    x = xtrain,\n",
    "    y = ytrain, \n",
    "    epochs=20, \n",
    "    batch_size=64,\n",
    "    validation_data=(xtest,ytest),\n",
    "),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff5e77-56e0-4bc2-8c46-f5b183bb3bae",
   "metadata": {},
   "source": [
    "#### Aparentemente eu estava vacilando.\n",
    "\n",
    "Garantir one_hot_encoding na saída dos dados foi fundamental para o LSTM  funcionar. Feito isso, deu bom o LSTM. Agora finalmente da pra finalizar esse notebook. \n",
    "\n",
    "Vou passar a limpo para outro notebook com um experimento único. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c819f9c4-092c-4559-9723-f6dd659d89d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
