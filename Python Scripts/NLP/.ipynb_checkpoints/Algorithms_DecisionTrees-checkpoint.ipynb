{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árvores de decisão\n",
    "\n",
    "Árvores de decisão tentam <b>classificar</b> os dados baseados em perguntas respondidas com valores booleanos.\n",
    "Todavia isso não restringe a trabalharmos com <b>dados categoricos</b> uma vez que podemos transformar os <b>dados numéricos</b> em <b>categóricos</b>\n",
    "\n",
    "Avaliando a precisão de cada dado com respeito a um rótulo Y, podemos ter indicios da eficácia do mesmo na classificação das amostras. Os preditores, isto é, característica que produzem folhas com multiclasse, denominamos impuras. Considere o exemplo a seguir.\n",
    "\n",
    "\"Dor no peito\"\n",
    "\n",
    "--T--> 105 Doentes/ 39 Saudáveis (Folha com 2 classes preditas)              \n",
    "\n",
    "--F--> 34 Doentes/ 125 Saudáveis (Folha com 2 classes preditas)\n",
    "\n",
    "Para determinar qual a melhor forma de montar a árvore, é preciso medir o comparar o grau de impureza dos nós.\n",
    "\n",
    "#### Método Gini para calculo de impureza\n",
    "\n",
    "O cálculo de impureza aqui acontece em cada folha (Isso pode ser facilmente generalizado caso haja multiplas classes).\n",
    "\n",
    "$$\n",
    "g(x) = 1 - P(y = 1 | x)^{2} + P(y = 0 | x)^{2}\n",
    "$$\n",
    "\n",
    "No caso de exemplo teremos (Para primeira folha, considerando $x=$ dor no peito) :\n",
    "\n",
    "$$\n",
    "1 - (\\frac{105}{144})^{2} + (\\frac{34}{144})^{2} = 0,395\n",
    "$$\n",
    "\n",
    "Na segunda folha temos $x = $ sem dor no peito:\n",
    "$$\n",
    "1 - (\\frac{34}{159})^{2} + (\\frac{125}{159})^{2} = 0,336\n",
    "$$\n",
    "\n",
    "Assim, a impureza total da característica X que equivale a pergunta \"Dor no peito\" é dada por:\n",
    "\n",
    "<b>OBS:</b> Considerando que x é uma característica binária, temos 1 - x o evento complementar.\n",
    "Exemplo, se x é \"dor no peito\" 1 - x é \"sem dor no peito\". Se considerarmos multiplas classes, o complemento seria qualquer evento que não seja x.\n",
    "A <b>amostragem de x é</b> o conjunto de amostras que passaram por aquele evento. \n",
    "\n",
    "\n",
    "$$\n",
    "\\bar i = \\sum_{x \\in X}\\frac{|Amostragem(x)|}{|Amostragem(x)| + |Amostragem(1 - x)|}g(x)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\bar i = \\frac{144}{144+159}0,395 + \\frac{159}{144+159}0,336 = 0,364\n",
    "$$\n",
    "\n",
    "O algoritmo procede então jogando as características com menos impureza para os nós superiores, e deixando as de maior impureza nos inferiores. Sempre priorizando, a partir da raiz usar os nós com maior pureza com relação a razão da distribuição dos dados.\n",
    "\n",
    "Se acontecer do próprio nó possuir menor impureza, ele se torna uma folha. Basicamente os passos são:\n",
    "\n",
    "1. Calcule $g(x) | \\forall x \\in X$.\n",
    "2. Se $g(current\\_node) < \\{g(x) | \\forall x \\in X\\}$, então o current_node é folha\n",
    "3. Caso contrário (Se separar os dados causar melhora), pegue o $g(x)$ com menor pontuação\n",
    "\n",
    "#### Vantagens de árvores de decisão\n",
    "\n",
    "1. Fácil de entender\n",
    "2. Útil na exploração dos dados\n",
    "3. Não requer muito tratamento dos dados\n",
    "4. Não é restrita pelo tipo de dado\n",
    "5. É não paramétrico, isto é, não precisa se pressupor nenhuma hipótese sobre o espaço da distribuição nem a estrutura da classificação.\n",
    "\n",
    "#### Desvantagens\n",
    "\n",
    "1. Overfitting, modelo extremamente influenciado pela amostragem\n",
    "2. Não aceita variáveis contínuas\n",
    "\n",
    "### Árvores de Regressão\n",
    "\n",
    "Enquanto árvores de decisão focam em prever dados categóricos, as de regressão preveem <b>dados contínuos</b>. Isto é, o valor obtido por nós folha é o valor médio dos Y de treinos que cairam naquela região. No caso das árvores de decisão usamos a moda(frequência).\n",
    "\n",
    "Ambos métodos utilizam a mesma abordagem gulosa top-down \"recursive binary sppliting\". É <b>gulosa</b> porque o algoritmo se preocupa somente com a divisão atual (espaço de busca local), e não as futuras.\n",
    "\n",
    "Em ambas a árvores, podem ser utilizadas poda para evitar overfitting.\n",
    "\n",
    "#### Chi-Square\n",
    "\n",
    "Consiste na soma dos quadrados da normal da diferença entre frequência observada e esperada.\n",
    "\n",
    "Chi-square = ((Actual – Expected)^2 / Expected)^1/2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
