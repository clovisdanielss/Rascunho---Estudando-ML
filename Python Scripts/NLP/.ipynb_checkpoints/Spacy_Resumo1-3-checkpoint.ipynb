{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando modelos\n",
    "\n",
    "Para carregar um modelo no spacy, usamos a função <code>spacy.load(string)</code>\n",
    "\n",
    "O parâmetro string consiste no modelo carregado.\n",
    "Referência de modelos podem ser vistas [aqui](https://spacy.io/usage)\n",
    "\n",
    "O método load retorna a instância do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os Docs\n",
    "\n",
    "<code>Doc</code> é a classe de documento básica do spacy. A referência dela é <code>spacy.tokens</code>.\n",
    "Um doc consiste em um texto processado. Ele é o retorno da execução do modelo.\n",
    "\n",
    "Um doc é dividido em tokens. Tokens são cada componente do texto processado. Um recorte do doc resulta em uma partição (Span). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Testing (96,)\n",
      "1 the (96,)\n",
      "2 model (96,)\n",
      "Testing the 0 2\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Testing the model\")\n",
    "for token in doc:\n",
    "    print(token.i,token.text, token.vector.shape)\n",
    "print(doc[0:2].text, doc[0:2].start, doc[0:2].end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashes\n",
    "\n",
    "Toda string é tratada na forma de hash por detrás do spacy. Por esse motivo, o código sempre será mais otimizado quando trabalharmos com objetos instânciados, isto é, processados pela tokenização do nlp.\n",
    "\n",
    "Sendo assim, evite trabalhar com strings até o último momento. O vocabulário que armazena esse dicionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12169968039091452412 Testing\n"
     ]
    }
   ],
   "source": [
    "hash = nlp.vocab.strings[\"Testing\"]\n",
    "name = nlp.vocab.strings[hash]\n",
    "print(hash, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construtores\n",
    "\n",
    "Aqui são apresentado os construtores de cada uma das entidades tratadas.\n",
    "Uma nota especial é que uma partição (<code>Span</code>) tem um atributo especial que é o rótulo (<code>label_</code>). Este é usado para definir as entidades existentes no documento.\n",
    "\n",
    "O doc em si, recebe um array de palavras e de booleanos, que indicam se existe ou não um espaço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example\n",
      "This is PARTICAO\n",
      "This\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "doc = Doc(nlp.vocab, [\"This\", \"is\", \"an\", \"example\"], [True, True, True, False])\n",
    "print(doc)\n",
    "span = Span(doc, 0, 2, label = \"PARTICAO\")\n",
    "print(span.text, span.label_)\n",
    "token = Token(nlp.vocab, doc, 0)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anotações linguísticas\n",
    "\n",
    "Algumas anotações linguísticas podem ser útil na análise do texto. Uma [referência](https://universaldependencies.org/docs/u/pos/) que pode ser útil é a demarcada. Ela (<code>.pos_</code>) indica os tipos de marcadores de classe gramátical universais. DET - adjunto adnominal\n",
    "\n",
    "O termo sintático em questão esta na variável <code>.dep_</code>. Esta por sua vez está classificando o This como um sujeito simples (nominal subject).\n",
    "\n",
    "Por ultimo, o <code>head</code> faz referênca ao \"pai\" daquele token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This DET nsubj is\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is a random phrase\")\n",
    "token = doc[0]\n",
    "print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entidades\n",
    "Com o modelo, muitas entidades já são treinadas para serem identificadas no texto. Nesse exemplo, temos a entidade ORG, de organização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Microsoft ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Upcoming iPhone X release date leaked as Apple reveals pre-orders. They arch-enemy, Microsoft, is mad.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matchers\n",
    "\n",
    "Um matcher é responsável por encontrar um determinado padrão dentro do texto.\n",
    "Ele difere de expressões regulares porque pode considerar semântica e sintaxe, conforme o processamento da frase.\n",
    "\n",
    "A regra recebe um nome, e um padrão, que consiste em uma Lista<Lista<Dicionário>>. Isto é, cada padrão é uma Lista<Dicionários>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enemy \n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "enemy_pattern = [[{\"LEMMA\":\"enemy\"}]]\n",
    "matcher.add(\"ENEMY\", enemy_pattern)\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match in matches:\n",
    "    print(doc[match[1]:match[2]].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade\n",
    "\n",
    "A similaridade entre Tokens, Spans ou até mesmo Docs pode ser fácilmente medida através do ângulo entre os vetores dos mesmos. \n",
    "\n",
    "Token -> Vetor. Span,Doc -> Média dos vetores dos tokens.\n",
    "\n",
    "Nesse caso, foi retornado um warning porque o presente modelo não possui nenhum tipo de embedding, isto é, vetorização para os tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-45c9d9acd8a0>:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  doc[0:3].similarity(doc[3:6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.250942"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0:3].similarity(doc[3:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "A pipeline consiste em um túnel de processamento onde o doc é passado. A imagem a seguir mostra um pipe padrão. \n",
    "A primeira camada, <strong>NÃO</strong> é considerada parte do pipeline, e é obrigatória (Tokenização). \n",
    "\n",
    "A segunda camada, Tagger, é responsável pelos marcadores de classe gramatical.\n",
    "A terceira camada, Parser, é responsável pela identificação dos termos sintáticos e dependências.\n",
    "A quarta, define as partições como entidades.\n",
    "\n",
    "As pipelines podem ser usadas para alterar o Doc ou usar o mesmo.\n",
    "\n",
    "![pipeline](https://course.spacy.io/pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "This is a phrase\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'my_pipeline']\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"my_pipeline\")\n",
    "def my_pipeline(doc):\n",
    "    print(len(doc))\n",
    "    return doc\n",
    "\n",
    "if(\"my_pipeline\" not in nlp.pipe_names):\n",
    "    nlp.add_pipe(\"my_pipeline\")\n",
    "doc = nlp(\"This is a phrase\")\n",
    "print(doc)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions\n",
    "\n",
    "Para atributos customizados, é possível trabalhar com o método <code>set_extension</code>. Esse método serve para <code>Doc</code>, <code>Span</code>, <code>Token</code>.\n",
    "\n",
    "Você pode definir:\n",
    "- Atributos: elementos com acesso direto, sem nenhum tipo de tratamento.\n",
    "- Propriedades: funções sem parâmetros, permitindo o acesso a um atributo com algum tipo de tratamento.\n",
    "- Métodos: funções parametrizadas.\n",
    "\n",
    "Importante saber que o primeiro parâmetro definido nas propriedades e métodos são do mesmo tipo da entidade base em que a extensão está sendo definida.\n",
    "\n",
    "Todo e qualquer extensão é acessada pelo atributo <code>_</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "False None\n",
      "True Black\n",
      "Black is equal Gray ? False\n"
     ]
    }
   ],
   "source": [
    "def get_has_color(token):\n",
    "    return token._.color != None\n",
    "\n",
    "def compare_color(token, tokenB):\n",
    "    return token._.color == tokenB._.color\n",
    "\n",
    "Token.set_extension(\"color\", default = None)\n",
    "Token.set_extension(\"has_color\", getter = get_has_color)\n",
    "Token.set_extension(\"compare_color\", method = compare_color)\n",
    "\n",
    "doc = nlp(\"This is an test\")\n",
    "print(doc[0]._.has_color, doc[0]._.color)\n",
    "doc[0]._.color = \"Black\"\n",
    "print(doc[0]._.has_color, doc[0]._.color)\n",
    "doc[1]._.color = \"Gray\"\n",
    "print(doc[0]._.color, \"is equal\", doc[1]._.color, \"?\", doc[0]._.compare_color(doc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
